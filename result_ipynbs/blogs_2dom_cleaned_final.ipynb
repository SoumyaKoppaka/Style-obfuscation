{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "import subprocess  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp37-cp37m-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp37-cp37m-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 216 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.29.1-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/soumyakoppaka/opt/anaconda3/envs/fp/lib/python3.7/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/soumyakoppaka/opt/anaconda3/envs/fp/lib/python3.7/site-packages (from matplotlib) (7.2.0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/soumyakoppaka/opt/anaconda3/envs/fp/lib/python3.7/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/soumyakoppaka/opt/anaconda3/envs/fp/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/soumyakoppaka/opt/anaconda3/envs/fp/lib/python3.7/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/soumyakoppaka/opt/anaconda3/envs/fp/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.29.1 kiwisolver-1.3.2 matplotlib-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doc_command(step, dire):\n",
    "\n",
    "    com = r\"\"\"../src/main.py \n",
    "            --dataset blogs_2dom_cleaned \n",
    "            --clean_mem_every 5 \n",
    "            --reset_output_dir \n",
    "            --classifier_dir ../pretrained_classifer/blogs_2dom_cleaned \n",
    "            --train_src_file ../data/blogs_2dom_cleaned/dev_drop_10.txt \n",
    "            --train_trg_file ../data/blogs_2dom_cleaned/dev_drop_10.docs \n",
    "            --dev_src_file ../data/blogs_2dom_cleaned/dev_drop_10.txt \n",
    "            --dev_trg_file ../data/blogs_2dom_cleaned/dev_drop_10.docs \n",
    "            --dev_trg_ref ../data/blogs_2dom_cleaned/dev_ref.txt \n",
    "            --src_vocab  ../data/blogs_2dom_cleaned/text.vocab \n",
    "            --trg_vocab  ../data/blogs_2dom_cleaned/doc.attr.vocab \n",
    "            --d_word_vec=128 \n",
    "            --d_model=512 \n",
    "            --log_every=100 \n",
    "            --eval_every=3000 \n",
    "            --ppl_thresh=10000 \n",
    "            --eval_bleu \n",
    "            --batch_size 32 \n",
    "            --valid_batch_size 128 \n",
    "            --patience 5 \n",
    "            --lr_dec 0.5 \n",
    "            --lr 0.001 \n",
    "            --dropout 0.3 \n",
    "            --max_len 10000 \n",
    "            --seed 0 \n",
    "            --beam_size 1 \n",
    "            --word_blank 0.2 \n",
    "            --word_dropout 0.1 \n",
    "            --word_shuffle 3 \n",
    "            --cuda \n",
    "            --anneal_epoch 5 \n",
    "            --temperature 0.01 \n",
    "            --klw 0.03 \n",
    "            --max_pool_k_size 1 \n",
    "            --bt \n",
    "            --lm \n",
    "            --gumbel_softmax \n",
    "            --avg_len \n",
    "            --run_classifier_doc_evaluation \n",
    "            --step {} \n",
    "            --no_styles 2 \n",
    "            --input_classifier_text ../{} \n",
    "            --input_doc_dict ../data/blogs_2dom_cleaned/doc.dict\"\"\".format(step,dire)\n",
    "    return com\n",
    "\n",
    "#print (proc.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_entropy_command(step, dire):\n",
    "\n",
    "    com = r\"\"\"../src/main.py \n",
    "        --dataset blogs_2dom_cleaned \n",
    "        --clean_mem_every 5 \n",
    "        --reset_output_dir\n",
    "        --classifier_dir ../pretrained_classifer/blogs_2dom_cleaned \n",
    "        --train_src_file ../data/blogs_2dom_cleaned/dev_drop_10.txt \n",
    "        --train_trg_file ../data/blogs_2dom_cleaned/dev_drop_10.attr \n",
    "        --dev_src_file ../data/blogs_2dom_cleaned/dev_drop_10.txt \n",
    "        --dev_trg_file ../data/blogs_2dom_cleaned/dev_drop_10.attr \n",
    "        --dev_trg_ref ../data/blogs_2dom_cleaned/dev_ref.txt \n",
    "        --src_vocab  ../data/blogs_2dom_cleaned/text.vocab \n",
    "        --trg_vocab  ../data/blogs_2dom_cleaned/attr_disc.vocab \n",
    "        --d_word_vec=128 \n",
    "        --d_model=512 \n",
    "        --log_every=100 \n",
    "        --eval_every=3000 \n",
    "        --ppl_thresh=10000 \n",
    "        --eval_bleu \n",
    "        --batch_size 32 \n",
    "        --valid_batch_size 128 \n",
    "        --patience 5 \n",
    "        --lr_dec 0.5 \n",
    "        --lr 0.001\n",
    "        --dropout 0.3\n",
    "        --max_len 10000\n",
    "        --seed 0 \n",
    "        --beam_size 1 \n",
    "        --word_blank 0.2 \n",
    "        --word_dropout 0.1 \n",
    "        --word_shuffle 3 \n",
    "        --cuda \n",
    "        --anneal_epoch 5 \n",
    "        --temperature 0.01 \n",
    "        --klw 0.03\n",
    "        --max_pool_k_size 1\n",
    "        --bt\n",
    "        --lm \n",
    "        --gumbel_softmax\n",
    "        --avg_len\n",
    "        --run_classifier_avg_certainty\n",
    "        --step {}\n",
    "        --no_styles 2\n",
    "        --input_classifier_text ../{}\"\"\".format(step,dire)\n",
    "    return com\n",
    "\n",
    "#print (proc.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confidence_ratio_command(step, dire):\n",
    "\n",
    "    com = r\"\"\"../src/main.py \n",
    "        --dataset blogs_2dom_cleaned \n",
    "        --clean_mem_every 5 \n",
    "        --reset_output_dir\n",
    "        --classifier_dir ../pretrained_classifer/blogs_2dom_cleaned \n",
    "        --train_src_file ../data/blogs_2dom_cleaned/dev_drop_10.txt \n",
    "        --train_trg_file ../data/blogs_2dom_cleaned/dev_drop_10.attr \n",
    "        --dev_src_file ../data/blogs_2dom_cleaned/dev_drop_10.txt \n",
    "        --dev_trg_file ../data/blogs_2dom_cleaned/dev_drop_10.attr \n",
    "        --dev_trg_ref ../data/blogs_2dom_cleaned/dev_ref.txt \n",
    "        --src_vocab  ../data/blogs_2dom_cleaned/text.vocab \n",
    "        --trg_vocab  ../data/blogs_2dom_cleaned/attr_disc.vocab \n",
    "        --d_word_vec=128 \n",
    "        --d_model=512 \n",
    "        --log_every=100 \n",
    "        --eval_every=3000 \n",
    "        --ppl_thresh=10000 \n",
    "        --eval_bleu \n",
    "        --batch_size 32 \n",
    "        --valid_batch_size 128 \n",
    "        --patience 5 \n",
    "        --lr_dec 0.5 \n",
    "        --lr 0.001\n",
    "        --dropout 0.3\n",
    "        --max_len 10000\n",
    "        --seed 0 \n",
    "        --beam_size 1 \n",
    "        --word_blank 0.2 \n",
    "        --word_dropout 0.1 \n",
    "        --word_shuffle 3 \n",
    "        --cuda \n",
    "        --anneal_epoch 5 \n",
    "        --temperature 0.01 \n",
    "        --klw 0.03\n",
    "        --max_pool_k_size 1\n",
    "        --bt\n",
    "        --lm \n",
    "        --gumbel_softmax\n",
    "        --avg_len\n",
    "        --run_classifier_count_certainty\n",
    "        --step {}\n",
    "        --no_styles 2\n",
    "        --input_classifier_text ../{}\"\"\".format(step,dire)\n",
    "    return com\n",
    "\n",
    "#print (proc.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gpt_command_dom(step, dire,dom,alph):\n",
    "\n",
    "    com = r\"\"\"/home/NAME/PROJECT/Deep-Learning/GPT2-HarryPotter-Training/examples/run_lm_finetuning.py\n",
    "        --output_dir=/home/NAME/PROJECT/Deep-Learning/GPT2-HarryPotter-Training/examples/output-dom{}\n",
    "        --model_type=gpt2\n",
    "        --model_name_or_path=gpt2-medium\n",
    "        --train_data_file=/home/NAME/PROJECT/style-pooling/{}/dev.trans_{}a{}\n",
    "        --do_eval\n",
    "        --eval_data_file=/home/NAME/PROJECT/style-pooling/{}/dev.trans_{}a{}\n",
    "        --block_size=200\n",
    "        --per_gpu_train_batch_size=1\"\"\".format(dom,dire,step,alph,dire,step,alph)\n",
    "    return com\n",
    "\n",
    "#print (proc.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gpt_command_original(step, dire):\n",
    "\n",
    "    com = r\"\"\"/home/name/NAME/PROJECT/Deep-Learning/GPT2-HarryPotter-Training/examples/run_lm_finetuning.py\n",
    "        --output_dir=/home/NAME/PROJECT/Deep-Learning/GPT2-HarryPotter-Training/examples/output-original/checkpoint-1\n",
    "        --model_type=gpt2\n",
    "        --model_name_or_path=gpt2-medium\n",
    "        --train_data_file=/home/NAME/PROJECT/style-pooling/{}/dev.trans_{}\n",
    "        --do_eval\n",
    "        --eval_data_file=/home//NAME/PROJECT/style-pooling/{}/dev.trans_{}\n",
    "        --block_size=200\n",
    "        --per_gpu_train_batch_size=1\"\"\".format(dire,step,dire,step)\n",
    "    return com\n",
    "\n",
    "#print (proc.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gpt_command_tuned(step, dire):\n",
    "\n",
    "    com = r\"\"\"/home/NAME/PROJECT/Deep-Learning/GPT2-HarryPotter-Training/examples/run_lm_finetuning.py\n",
    "        --output_dir=/home/NAME/PROJECT/Deep-Learning/GPT2-HarryPotter-Training/examples/output/checkpoint-290000\n",
    "        --model_type=gpt2\n",
    "        --model_name_or_path=gpt2-medium\n",
    "        --train_data_file=/home/NAME/PROJECT/style-pooling/{}/dev.trans_{}\n",
    "        --do_eval\n",
    "        --eval_data_file=/home/NAME/PROJECT/style-pooling/{}/dev.trans_{}\n",
    "        --block_size=200\n",
    "        --per_gpu_train_batch_size=1\"\"\".format(dire,step,dire,step)\n",
    "    return com\n",
    "\n",
    "#print (proc.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_cmd(step, dire):\n",
    "\n",
    "    com = r\"\"\"split -l 19374 /home/NAME/PROJECT/style-pooling/{}/dev.trans_{} /home/NAME/PROJECT/style-pooling/{}/dev.trans_{}\"\"\".format(dire,step,dire,step)\n",
    "    return com\n",
    "\n",
    "#print (proc.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lexi_cmd(step, dire):\n",
    "\n",
    "    com = r\"\"\"../scripts/lexical_div.py --input /home/NAME/PROJECT/style-pooling/{}/dev.trans_{}\"\"\".format(dire,step)\n",
    "    return com\n",
    "\n",
    "#print (proc.communicate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sum + onelm + min not distinguished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../place log path here/stdout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rg/167nrm1s43nbt6tm6_4w38080000gn/T/ipykernel_91998/2990525419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlog_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlog_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/stdout\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mval_step\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mclassifier_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../place log path here/stdout'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "val_step =[]\n",
    "classifier_acc = []\n",
    "n_acc = []\n",
    "bt_acc = []\n",
    "total_loss=[]\n",
    "neg_ELBO = []\n",
    "KL_loss=[]\n",
    "\n",
    "confidence_accuracy = []\n",
    "confidence_ratio = []\n",
    "\n",
    "\n",
    "cls_sent = []\n",
    "maj_doc = []\n",
    "sum_doc = []\n",
    "\n",
    "log_paths=[\n",
    "\"place log path here\"\n",
    "]\n",
    "for log_path in log_paths:   \n",
    "    with open(\"../\"+log_path+\"/stdout\") as f:\n",
    "        val_step =[]\n",
    "        classifier_acc = []\n",
    "        n_acc = []\n",
    "        bt_acc = []\n",
    "        total_loss=[]\n",
    "        neg_ELBO = []\n",
    "        KL_loss=[]\n",
    "\n",
    "        cls_sent = []\n",
    "        maj_doc = []\n",
    "        sum_doc = []\n",
    "        entorpy = []\n",
    "        diff_per = []\n",
    "\n",
    "        gpt_original =[]\n",
    "        gpt_dom0_0 =[]\n",
    "        gpt_dom0_1 =[]\n",
    "        gpt_dom1_0 =[]\n",
    "        gpt_dom1_1 =[]\n",
    "        gpt_tuned =[]\n",
    "\n",
    "        lex_div = []\n",
    "        unique_words = []\n",
    "\n",
    "        confidence_accuracy = []\n",
    "        confidence_ratio = []\n",
    "        for i, line in enumerate(f):\n",
    "            if (\"classifier_acc=\" in line):\n",
    "                list_acc = re.findall(\"\\d+\\.\\d+\", line)\n",
    "                #print(list_acc)\n",
    "                classifier_acc.append(float(list_acc[0]))\n",
    "            elif (\"val_step\" in line):\n",
    "                list_arguments = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "                #print(list_arguments)\n",
    "                if (int(list_arguments[0]) >33000):\n",
    "                    continue\n",
    "                elif (int(list_arguments[0]) < 24000):\n",
    "                    continue\n",
    "                elif (int(list_arguments[0])% 1500 != 0):\n",
    "                    continue\n",
    "                val_step.append(int(list_arguments[0]))\n",
    "                total_loss.append(float(list_arguments[1]))\n",
    "                neg_ELBO.append(float(list_arguments[2]))\n",
    "                KL_loss.append(float(list_arguments[3]))\n",
    "                n_acc.append(float(list_arguments[6])*100)\n",
    "                bt_acc.append(float(list_arguments[7])*100)\n",
    "                \n",
    "                ###get acc nums\n",
    "                com = make_doc_command(str(val_step[-1]),log_path)\n",
    "                proc = subprocess.Popen([\"/home/name/anaconda3/envs/py37/bin/python\"]+(com.replace('\\n','').split()), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "                outp = (proc.communicate())\n",
    "                list_arguments = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", str(outp[0]))\n",
    "                sum_doc.append(float(list_arguments[-3]))\n",
    "                maj_doc.append(float(list_arguments[-2]))\n",
    "                cls_sent.append(float(list_arguments[-1]))\n",
    "                #print(outp)\n",
    "                ## get entropy\n",
    "                com = make_entropy_command(str(val_step[-1]),log_path)\n",
    "                proc = subprocess.Popen([\"/home/name/anaconda3/envs/py37/bin/python\"]+(com.replace('\\n','').split()), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "                outp = (proc.communicate())\n",
    "                list_arguments = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", str(outp[0]))\n",
    "                diff_per.append(float(list_arguments[-2]))\n",
    "                entorpy.append(float(list_arguments[-1]))\n",
    "                ## get confidence ratio\n",
    "                com = make_confidence_ratio_command(str(val_step[-1]),log_path)\n",
    "                proc = subprocess.Popen([\"/home/name/anaconda3/envs/py37/bin/python\"]+(com.replace('\\n','').split()), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "                outp = (proc.communicate())\n",
    "                list_arguments = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", str(outp[0]))\n",
    "                confidence_accuracy.append(float(list_arguments[-2]))\n",
    "                confidence_ratio.append(float(list_arguments[-1]))               \n",
    "                ## get lex div\n",
    "                com = make_lexi_cmd(str(val_step[-1]),log_path)\n",
    "                proc = subprocess.Popen([\"/home/name/anaconda3/envs/py37/bin/python\"]+(com.replace('\\n','').split()), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "                outp = (proc.communicate())\n",
    "                list_arguments = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", str(outp[0]))\n",
    "                unique_words.append(float(list_arguments[-2]))\n",
    "                lex_div.append(float(list_arguments[-1]))\n",
    "                ##gpt\n",
    "                ##### get gpt-original\n",
    "                com = make_gpt_command_original(str(val_step[-1]),log_path)\n",
    "                proc = subprocess.Popen(\"export CUDA_VISIBLE_DEVICES=2;/home/name/anaconda3/envs/py37/bin/python \" +\" \".join(com.replace('\\n','').split()), cwd='/home/name/PROJECT/Deep-Learning/GPT2-HarryPotter-Training/examples', stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True,  executable = '/bin/sh')\n",
    "            \n",
    "                outp = (proc.communicate())\n",
    "                list_arguments = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", str(outp[0]))\n",
    "                gpt_original.append(float(list_arguments[-1]))\n",
    "                #print(outp)\n",
    "                #print(float(list_arguments[-1]))\n",
    "\n",
    "\n",
    "  \n",
    "                #print(list_arguments)\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i) for i in val_step])+\"\\\",\\\",\\\")\")\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i) for i in bt_acc])+\"\\\",\\\",\\\")\")\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i*100) for i in sum_doc])+\"\\\",\\\",\\\")\")\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i*100) for i in maj_doc])+\"\\\",\\\",\\\")\")\n",
    "        #print(\"=SPLIT(\\\"\"+','.join([str(i*100) for i in classifier_acc])+\"\\\",\\\",\\\")\")\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i*100) for i in cls_sent])+\"\\\",\\\",\\\")\")\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i) for i in entorpy])+\"\\\",\\\",\\\")\")\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i*100) for i in diff_per])+\"\\\",\\\",\\\")\")\n",
    "        ##\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i*100) for i in confidence_accuracy])+\"\\\",\\\",\\\")\")\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i*100) for i in confidence_ratio])+\"\\\",\\\",\\\")\")\n",
    "        #gpt\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i) for i in gpt_original])+\"\\\",\\\",\\\")\")\n",
    "\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i*100) for i in lex_div])+\"\\\",\\\",\\\")\")\n",
    "        print(\"=SPLIT(\\\"\"+','.join([str(i) for i in unique_words])+\"\\\",\\\",\\\")\")\n",
    "\n",
    "\n",
    "        print(\"********************\")\n",
    "\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "print(len(val_step), len(n_acc), len(bt_acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
